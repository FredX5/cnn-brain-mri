{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":377107,"sourceType":"datasetVersion","datasetId":165566}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/faridjamalov/https-www-kaggle-com-code-faridjamalov-cnn-br?scriptVersionId=264003975\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport tensorflow as tf\n\n# ------------------------------\n# Step 0: Suppress unnecessary TF warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Hides INFO and WARNING logs\n# ------------------------------\n\n# ------------------------------\n# Step 1: Inspect Kaggle input directory\ndata_dir = \"/kaggle/input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset\"\n\nprint(\"Classes:\", os.listdir(data_dir))\nfor folder in os.listdir(data_dir):\n    folder_path = os.path.join(data_dir, folder)\n    if os.path.isdir(folder_path):\n        print(folder, \":\", len(os.listdir(folder_path)))\n# ------------------------------\n\n# ------------------------------\n# Step 2: Check GPU availability\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Enable memory growth to avoid pre-allocating full GPU memory\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"GPU(s) detected: {gpus}\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"No GPU detected. Using CPU.\")\n\n# Optional: confirm TensorFlow version and available devices\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Available devices:\", tf.config.list_physical_devices())\n# ------------------------------\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:01:38.795898Z","iopub.execute_input":"2025-09-25T17:01:38.796179Z","iopub.status.idle":"2025-09-25T17:01:38.843012Z","shell.execute_reply.started":"2025-09-25T17:01:38.796159Z","shell.execute_reply":"2025-09-25T17:01:38.841654Z"}},"outputs":[{"name":"stdout","text":"Classes: ['no', 'yes']\nno : 98\nyes : 155\nNo GPU detected. Using CPU.\nTensorFlow version: 2.18.0\nAvailable devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n","output_type":"stream"},{"name":"stderr","text":"2025-09-25 17:01:38.837728: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"What this code does\nSuppress unnecessary warnings\n\nPrevents TensorFlow from printing lots of INFO and WARNING messages so the output is cleaner.\n\nInspect the dataset\n\nLooks into the Kaggle input folder.\n\nIdentifies the classes (e.g., tumor vs. no tumor) by checking folder names.\n\nCounts how many images are available in each class to understand dataset balance.\n\nCheck GPU availability\n\nDetermines whether a GPU is available for faster training.\n\nSets GPU memory to grow dynamically instead of reserving all memory upfront.\n\nFalls back to CPU if no GPU is found.\n\nConfirm TensorFlow version\n\nHelps ensure your code runs with the correct library version and is compatible with the system setup.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:01:53.900273Z","iopub.execute_input":"2025-09-25T17:01:53.900739Z","iopub.status.idle":"2025-09-25T17:01:53.905534Z","shell.execute_reply.started":"2025-09-25T17:01:53.900716Z","shell.execute_reply":"2025-09-25T17:01:53.904298Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# ImageDataGenerator with augmentation and normalization\ndatagen = ImageDataGenerator(\n    rescale=1./255,           # Normalize pixel values\n    shear_range=0.2,          # Random shear\n    zoom_range=0.2,           # Random zoom\n    horizontal_flip=True,     # Random horizontal flip\n    validation_split=0.2      # 20% for validation\n)\n\n# Training generator\ntrain_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(150, 150),   # Resize images\n    batch_size=32,\n    class_mode='binary',      # Binary classification\n    subset='training',\n    shuffle=True\n)\n\n# Validation generator\nvalidation_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation',\n    shuffle=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:02:00.373946Z","iopub.execute_input":"2025-09-25T17:02:00.374292Z","iopub.status.idle":"2025-09-25T17:02:00.771574Z","shell.execute_reply.started":"2025-09-25T17:02:00.374256Z","shell.execute_reply":"2025-09-25T17:02:00.770795Z"}},"outputs":[{"name":"stdout","text":"Found 203 images belonging to 2 classes.\nFound 50 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Summary of what this does:\n\nNormalizes images to the 0–1 range.\n\nApplies data augmentation (shear, zoom, horizontal flip) to make the model more robust.\n\nSplits the dataset into training (80%) and validation (20%).\n\nPrepares iterators (generators) that feed images to the model in batches during training.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\nimport seaborn as sns\nimport cv2\n\n# -----------------------------\n# 1️⃣ Data Generators (senin kodundan)\ndata_dir = \"/kaggle/input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset\"\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    subset='training',\n    shuffle=True\n)\n\nvalidation_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation',\n    shuffle=False\n)\n\n# -----------------------------\n# 2️⃣ CNN Model\nmodel = Sequential([\n    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n    MaxPooling2D(2,2),\n    \n    Conv2D(64, (3,3), activation='relu'),\n    MaxPooling2D(2,2),\n    \n    Conv2D(128, (3,3), activation='relu'),\n    MaxPooling2D(2,2),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# -----------------------------\n# 3️⃣ Train the model\nhistory = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=15\n)\n\n# -----------------------------\n# 4️⃣ Plot Accuracy & Loss\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.legend()\nplt.title('Accuracy')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.title('Loss')\nplt.show()\n\n# -----------------------------\n# 5️⃣ Confusion Matrix & Classification Report\nval_steps = validation_generator.samples // validation_generator.batch_size + 1\npreds = model.predict(validation_generator, steps=val_steps)\ny_pred = (preds > 0.5).astype(int).ravel()\ny_true = validation_generator.classes\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=validation_generator.class_indices, yticklabels=validation_generator.class_indices)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nprint(classification_report(y_true, y_pred, target_names=validation_generator.class_indices.keys()))\n\n# -----------------------------\n# 6️⃣ Grad-CAM Example (1 test image)\ndef get_gradcam(model, img_array, layer_name='conv2d_2'):\n    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        loss = predictions[:,0]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\n# Load a test image from validation set\nimg_path = validation_generator.filepaths[0]\nimg = tf.keras.utils.load_img(img_path, target_size=(150,150))\nimg_array = tf.keras.utils.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0) / 255.0\n\nheatmap = get_gradcam(model, img_array)\n\nplt.imshow(img)\nplt.imshow(heatmap, cmap='jet', alpha=0.5)  # Overlay heatmap\nplt.title(\"Grad-CAM\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:02:07.879848Z","iopub.execute_input":"2025-09-25T17:02:07.880181Z","iopub.status.idle":"2025-09-25T17:02:39.651669Z","shell.execute_reply.started":"2025-09-25T17:02:07.880158Z","shell.execute_reply":"2025-09-25T17:02:39.649873Z"}},"outputs":[{"name":"stdout","text":"Found 203 images belonging to 2 classes.\nFound 50 images belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m4,735,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,735,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,828,481\u001b[0m (18.42 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,828,481</span> (18.42 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,828,481\u001b[0m (18.42 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,828,481</span> (18.42 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6048 - loss: 0.6371 - val_accuracy: 0.7000 - val_loss: 0.5999\nEpoch 2/15\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.6783 - loss: 0.6092 - val_accuracy: 0.7000 - val_loss: 0.6075\nEpoch 3/15\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 890ms/step - accuracy: 0.7303 - loss: 0.5936 - val_accuracy: 0.7600 - val_loss: 0.5219\nEpoch 4/15\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1739703800.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# 3️⃣ Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"This part of the code is preparing the dataset for model training and validation. It normalizes image pixel values and applies data augmentation (shear, zoom, horizontal flip) to improve model generalization. The ImageDataGenerator creates iterators (generators) that automatically load images in batches, resize them to a consistent shape, and split the data into training (80%) and validation (20%), making the workflow efficient for feeding images into a CNN model.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ny_pred = (model.predict(validation_generator) > 0.5).astype(int).ravel()\ny_true = validation_generator.classes\n\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\nprint(classification_report(y_true, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:02:48.200174Z","iopub.execute_input":"2025-09-25T17:02:48.200795Z","iopub.status.idle":"2025-09-25T17:02:50.189977Z","shell.execute_reply.started":"2025-09-25T17:02:48.200763Z","shell.execute_reply":"2025-09-25T17:02:50.188734Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step\n              precision    recall  f1-score   support\n\n           0       0.67      0.53      0.59        19\n           1       0.74      0.84      0.79        31\n\n    accuracy                           0.72        50\n   macro avg       0.70      0.68      0.69        50\nweighted avg       0.71      0.72      0.71        50\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"This code is evaluating the performance of a trained model on the validation dataset.\n\nmodel.predict(validation_generator) generates predicted probabilities for each image.\n\n(> 0.5).astype(int) converts probabilities to binary class labels.\n\nconfusion_matrix shows the counts of true positives, true negatives, false positives, and false negatives, helping visualize where the model makes mistakes.\n\nsns.heatmap plots the confusion matrix for easy interpretation.\n\nclassification_report provides detailed metrics like precision, recall, F1-score, and accuracy for each class.\n Usability:\nThis helps in assessing how well the model performs, identifying biases, and guiding further improvements in training or preprocessing.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_true, y_pred, target_names=validation_generator.class_indices.keys()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:02:56.652459Z","iopub.execute_input":"2025-09-25T17:02:56.652848Z","iopub.status.idle":"2025-09-25T17:02:56.669681Z","shell.execute_reply.started":"2025-09-25T17:02:56.652824Z","shell.execute_reply":"2025-09-25T17:02:56.668523Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          no       0.67      0.53      0.59        19\n         yes       0.74      0.84      0.79        31\n\n    accuracy                           0.72        50\n   macro avg       0.70      0.68      0.69        50\nweighted avg       0.71      0.72      0.71        50\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"img_path = validation_generator.filepaths[0]\n# Load image, preprocess, get heatmap, overlay\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:02:59.680242Z","iopub.execute_input":"2025-09-25T17:02:59.680582Z","iopub.status.idle":"2025-09-25T17:02:59.685985Z","shell.execute_reply.started":"2025-09-25T17:02:59.680558Z","shell.execute_reply":"2025-09-25T17:02:59.684769Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport tensorflow as tf\n\n# -----------------------------\n# 1️⃣ Tahminleri al\nval_steps = validation_generator.samples // validation_generator.batch_size + 1\ny_pred_probs = model.predict(validation_generator, steps=val_steps)\ny_pred = (y_pred_probs > 0.5).astype(int).ravel()\ny_true = validation_generator.classes\n\n# -----------------------------\n# 2️⃣ Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=validation_generator.class_indices, \n            yticklabels=validation_generator.class_indices)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# -----------------------------\n# 3️⃣ Classification Report\nprint(\"Classification Report:\\n\")\nprint(classification_report(y_true, y_pred, target_names=validation_generator.class_indices.keys()))\n\n# -----------------------------\n# 4️⃣ Grad-CAM Fonksiyonu\ndef get_gradcam(model, img_array, layer_name='conv2d_2'):\n    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        loss = predictions[:,0]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\n# -----------------------------\n# 5️⃣ Örnek Grad-CAM Görselleştirme\nimg_path = validation_generator.filepaths[0]\nimg = tf.keras.utils.load_img(img_path, target_size=(150,150))\nimg_array = tf.keras.utils.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0) / 255.0\n\nheatmap = get_gradcam(model, img_array)\n\nplt.imshow(img)\nplt.imshow(heatmap, cmap='jet', alpha=0.5)  # Overlay heatmap\nplt.title(\"Grad-CAM\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:03:06.754618Z","iopub.execute_input":"2025-09-25T17:03:06.754932Z","iopub.status.idle":"2025-09-25T17:03:08.231759Z","shell.execute_reply.started":"2025-09-25T17:03:06.754912Z","shell.execute_reply":"2025-09-25T17:03:08.229747Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Classification Report:\n\n              precision    recall  f1-score   support\n\n          no       0.75      0.63      0.69        19\n         yes       0.79      0.87      0.83        31\n\n    accuracy                           0.78        50\n   macro avg       0.77      0.75      0.76        50\nweighted avg       0.78      0.78      0.78        50\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2603447318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gradcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2603447318.py\u001b[0m in \u001b[0;36mget_gradcam\u001b[0;34m(model, img_array, layer_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# 4️⃣ Grad-CAM Fonksiyonu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_gradcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv2d_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mgrad_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mconv_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_tensors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m_get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0;34mf\"The layer {self.name} has never been called \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf\"and thus has no defined {attr_name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: The layer sequential has never been called and thus has no defined output."],"ename":"AttributeError","evalue":"The layer sequential has never been called and thus has no defined output.","output_type":"error"}],"execution_count":18},{"cell_type":"markdown","source":"This code is focused on evaluating and interpreting the model’s predictions on the validation set:\n\nPredictions (y_pred)\n\nThe model predicts probabilities for each validation image.\n\nProbabilities are converted to binary class labels (0 or 1) for evaluation.\n\nConfusion Matrix\n\nVisualizes the model’s correct and incorrect predictions.\n\nsns.heatmap makes it easy to see true positives, true negatives, false positives, and false negatives.\n\nClassification Report\n\nProvides precision, recall, F1-score, and support for each class.\n\nHelps assess overall performance and class-specific performance.\n\nGrad-CAM Placeholder\n\nIndicates the next step would be to implement Grad-CAM, a technique to visualize which parts of an image the CNN focuses on when making predictions.\n\n Usability:\nThis section is essential for model evaluation and explainability, helping you identify strengths, weaknesses, and interpret how the CNN makes decisions.","metadata":{}},{"cell_type":"code","source":"# A simple Grad-CAM function adapted for Keras\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:03:17.821639Z","iopub.execute_input":"2025-09-25T17:03:17.822035Z","iopub.status.idle":"2025-09-25T17:03:17.830088Z","shell.execute_reply.started":"2025-09-25T17:03:17.822009Z","shell.execute_reply":"2025-09-25T17:03:17.828477Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"How it works:\n\nBuilds a gradient model that outputs both the last conv layer’s activations and the model’s predictions.\n\nComputes the gradient of the predicted class with respect to the last conv layer.\n\nAverages gradients across width and height to get importance weights for each feature map.\n\nComputes a weighted sum of feature maps to produce the class activation heatmap.\n\nNormalizes the heatmap between 0 and 1 for visualization.\n\n✅ Usability:\n\nAllows interpretation of CNN decisions by highlighting image regions influencing the prediction.\n\nUseful for debugging, explaining misclassifications, and improving trust in model predictions, especially in medical imaging tasks like brain tumor detection.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndata_dir = \"/kaggle/input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset\"\n\nIMG_SIZE = 150\nBATCH_SIZE = 32\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    shear_range=0.2,\n    zoom_range=0.2,\n    validation_split=0.2  # 80% train / 20% validation\n)\n\ntrain_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',   # binary since only yes/no\n    subset='training'\n)\n\nvalidation_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:09:44.240351Z","iopub.execute_input":"2025-09-25T17:09:44.241162Z","iopub.status.idle":"2025-09-25T17:09:44.509836Z","shell.execute_reply.started":"2025-09-25T17:09:44.241134Z","shell.execute_reply":"2025-09-25T17:09:44.508595Z"}},"outputs":[{"name":"stdout","text":"Found 203 images belonging to 2 classes.\nFound 50 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"This tells TensorFlow where your MRI images are stored.\n\nInside, you have two folders: no (no tumor) and yes (tumor).\n\nThis folder structure is perfect for flow_from_directory.","metadata":{}},{"cell_type":"code","source":"# Run a forward pass once so that layer outputs exist\n_ = model.predict(img)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:12:28.794541Z","iopub.execute_input":"2025-09-25T17:12:28.79491Z","iopub.status.idle":"2025-09-25T17:12:29.024027Z","shell.execute_reply.started":"2025-09-25T17:12:28.794886Z","shell.execute_reply":"2025-09-25T17:12:29.023082Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\n\n# Pick one validation image\nsample_img, _ = next(validation_generator)\nimg = np.expand_dims(sample_img[0], axis=0)\n\n# Choose last convolutional layer name from model.summary()\nlast_conv_layer_name = [layer.name for layer in model.layers if 'conv' in layer.name][-1]\nprint(\"Last conv layer:\", last_conv_layer_name)\n\n# Build grad model\ngrad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n\nwith tf.GradientTape() as tape:\n    conv_outputs, predictions = grad_model(img)\n    pred_index = tf.argmax(predictions[0])\n    loss = predictions[:, pred_index]\n\n# Compute gradients\ngrads = tape.gradient(loss, conv_outputs)\npooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n# Weight conv layer outputs\nconv_outputs = conv_outputs[0]\nheatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1).numpy()\n\n# Normalize heatmap\nheatmap = np.maximum(heatmap, 0)\nheatmap /= np.max(heatmap)\n\n# Overlay heatmap on original image\nheatmap = cv2.resize(heatmap, (IMG_SIZE, IMG_SIZE))\nplt.imshow(sample_img[0])\nplt.imshow(heatmap, cmap=\"jet\", alpha=0.5)\nplt.axis(\"off\")\nplt.title(\"Grad-CAM\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:12:42.166253Z","iopub.execute_input":"2025-09-25T17:12:42.166548Z","iopub.status.idle":"2025-09-25T17:12:42.351235Z","shell.execute_reply.started":"2025-09-25T17:12:42.166527Z","shell.execute_reply":"2025-09-25T17:12:42.349969Z"}},"outputs":[{"name":"stdout","text":"Last conv layer: conv2d_2\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/468186840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Build grad model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgrad_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_conv_layer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_tensors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m_get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0;34mf\"The layer {self.name} has never been called \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf\"and thus has no defined {attr_name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: The layer sequential has never been called and thus has no defined output."],"ename":"AttributeError","evalue":"The layer sequential has never been called and thus has no defined output.","output_type":"error"}],"execution_count":31},{"cell_type":"markdown","source":"The feature maps from conv2d_2\n\nThe final predictions\n\nThis layer is crucial because:\n\nConvolutional layers retain spatial information, unlike dense layers.\n\nGrad-CAM uses the gradients flowing back from the predicted class to see which regions in these feature maps are most important.\n\nEverything that follows in my code—computing gradients, pooling them, weighting feature maps, and creating the heatmap—is based on this conv2d_2 layer.","metadata":{}}]}